{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsnam95/my/blob/main/cluster_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[home](http://www.brandonrose.org)"
      ],
      "metadata": {
        "id": "HorDuoXxX5en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Document Clustering with Python"
      ],
      "metadata": {
        "id": "CygnJNw2X5eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/brandomr/document_cluster/blob/master/header_short.jpg?raw=1'>\n",
        "\n",
        "In this guide, I will explain how to cluster a set of documents using Python. My motivating example is to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list). See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the example. This guide covers:\n",
        "\n",
        "<ul>\n",
        "<li> tokenizing and stemming each synopsis\n",
        "<li> transforming the corpus into vector space using [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
        "<li> calculating cosine distance between each document as a measure of similarity\n",
        "<li> clustering the documents using the [k-means algorithm](http://en.wikipedia.org/wiki/K-means_clustering)\n",
        "<li> using [multidimensional scaling](http://en.wikipedia.org/wiki/Multidimensional_scaling) to reduce dimensionality within the corpus\n",
        "<li> plotting the clustering output using [matplotlib](http://matplotlib.org/) and [mpld3](http://mpld3.github.io/)\n",
        "<li> conducting a hierarchical clustering on the corpus using [Ward clustering](http://en.wikipedia.org/wiki/Ward%27s_method)\n",
        "<li> plotting a Ward dendrogram\n",
        "<li> topic modeling using [Latent Dirichlet Allocation (LDA)](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
        "</ul>"
      ],
      "metadata": {
        "id": "R93dHU40X5er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Contents"
      ],
      "metadata": {
        "id": "IimIHGcjX5es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li>[Stopwords, stemming, and tokenization](#Stopwords,-stemming,-and-tokenizing)\n",
        "<li>[Tf-idf and document similarity](#Tf-idf-and-document-similarity)\n",
        "<li>[K-means clustering](#K-means-clustering)\n",
        "<li>[Multidimensional scaling](#Multidimensional-scaling)\n",
        "<li>[Visualizing document clusters](#Visualizing-document-clusters)\n",
        "<li>[Hierarchical document clustering](#Hierarchical-document-clustering)\n",
        "<li>[Latent Dirichlet Allocation (LDA)](#Latent-Dirichlet-Allocation)\n",
        "</ul>"
      ],
      "metadata": {
        "id": "6_eNlthUX5es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But first, I import everything I am going to need up front"
      ],
      "metadata": {
        "id": "0uhNLrHCX5es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "from sklearn import feature_extraction\n",
        "!pip install mpld3\n",
        "import mpld3"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpld3\n",
            "  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from mpld3) (3.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mpld3) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->mpld3) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (4.39.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mpld3) (0.11.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mpld3) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mpld3) (1.16.0)\n",
            "Installing collected packages: mpld3\n",
            "Successfully installed mpld3-0.5.9\n"
          ]
        }
      ],
      "metadata": {
        "id": "mJXJ1Sk2X5et",
        "outputId": "e4a17cde-1164-4354-9d04-156c52f80cbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords, stemming, and tokenizing"
      ],
      "metadata": {
        "id": "mVpQBp8gX5eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import three lists: titles, links and wikipedia synopses\n",
        "titles = open('title_list.txt').read().split('\\n')\n",
        "#ensures that only the first 100 are read in\n",
        "titles = titles[:100]\n",
        "\n",
        "links = open('link_list_imdb.txt').read().split('\\n')\n",
        "links = links[:100]\n",
        "\n",
        "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_wiki = synopses_wiki[:100]\n",
        "\n",
        "synopses_clean_wiki = []\n",
        "for text in synopses_wiki:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_wiki.append(text)\n",
        "\n",
        "synopses_wiki = synopses_clean_wiki\n",
        "    \n",
        "    \n",
        "genres = open('genres_list.txt').read().split('\\n')\n",
        "genres = genres[:100]\n",
        "\n",
        "print(str(len(titles)) + ' titles')\n",
        "print(str(len(links)) + ' links')\n",
        "print(str(len(synopses_wiki)) + ' synopses')\n",
        "print(str(len(genres)) + ' genres')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 titles\n",
            "100 links\n",
            "100 synopses\n",
            "100 genres\n"
          ]
        }
      ],
      "metadata": {
        "id": "YhI2gxBTX5eu",
        "outputId": "2889a9ae-be56-4bd7-ee3b-121863f0bca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_imdb = synopses_imdb[:100]\n",
        "\n",
        "synopses_clean_imdb = []\n",
        "\n",
        "for text in synopses_imdb:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_imdb.append(text)\n",
        "\n",
        "synopses_imdb = synopses_clean_imdb"
      ],
      "outputs": [],
      "metadata": {
        "id": "JIe8zfjOX5ev"
      },
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "synopses = []\n",
        "\n",
        "for i in range(len(synopses_wiki)):\n",
        "    item = synopses_wiki[i] + synopses_imdb[i]\n",
        "    synopses.append(item)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qBxlpX5WX5ew"
      },
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
        "ranks = []\n",
        "\n",
        "for i in range(0,len(titles)):\n",
        "    ranks.append(i)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rKDTJcq3X5ew"
      },
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is focused on defining some functions to manipulate the synopses. First, I load [NLTK's](http://www.nltk.org/) list of English stop words. [Stop words](http://en.wikipedia.org/wiki/Stop_words) are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. I'm sure there are much better explanations of this out there."
      ],
      "metadata": {
        "id": "YKzNJ_RjX5ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load nltk's English stopwords as variable called 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "metadata": {
        "id": "T8spOHk_X5ex",
        "outputId": "5dec6f95-d658-47a2-c76c-1b00f2378f0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I import the [Snowball Stemmer](http://snowball.tartarus.org/) which is actually part of NLTK. [Stemming](http://en.wikipedia.org/wiki/Stemming) is just the process of breaking a word down into its root."
      ],
      "metadata": {
        "id": "P8y82RPSX5ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "43JPzCDKX5ex"
      },
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Below I define two functions:\n",
        "\n",
        "<ul>\n",
        "<li> *tokenize_and_stem*: tokenizes (splits the synopsis into a list of its respective words (or tokens) and also stems each token <li> *tokenize_only*: tokenizes the synopsis only\n",
        "</ul>\n",
        "\n",
        "I use both these functions to create a dictionary which becomes important in case I want to use stems for an algorithm, but later convert stems back to their full words for presentation purposes. Guess what, I do want to do that!\n",
        "\n"
      ],
      "metadata": {
        "id": "N-nFhvhvX5ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "\n",
        "def tokenize_only(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "outputs": [],
      "metadata": {
        "id": "litqG7S-X5ey"
      },
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below I use my stemming/tokenizing and tokenizing functions to iterate over the list of synopses to create two vocabularies: one stemmed and one only tokenized. "
      ],
      "metadata": {
        "id": "NYUHHBvTX5ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "totalvocab_stemmed = []\n",
        "totalvocab_tokenized = []\n",
        "for i in synopses:\n",
        "    allwords_stemmed = tokenize_and_stem(i)\n",
        "    totalvocab_stemmed.extend(allwords_stemmed)\n",
        "    \n",
        "    allwords_tokenized = tokenize_only(i)\n",
        "    totalvocab_tokenized.extend(allwords_tokenized)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "metadata": {
        "id": "zeaKiYRRX5ey",
        "outputId": "ac0c9faa-2ffd-4eb2-af10-36e8a77651e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using these two lists, I create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. The benefit of this is it provides an efficient way to look up a stem and return a full token. The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc. For my purposes this is fine--I'm perfectly happy returning the first token associated with the stem I need to look up."
      ],
      "metadata": {
        "id": "tyAP7qhvX5ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
      ],
      "outputs": [],
      "metadata": {
        "id": "eUZOYgDOX5ez"
      },
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tf-idf and document similarity"
      ],
      "metadata": {
        "id": "nEoYbgnMX5ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402' align='right' style=\"margin-left:10px\">\n",
        "\n",
        "Here, I define term frequency-inverse document frequency (tf-idf) vectorizer parameters and then convert the *synopses* list into a tf-idf matrix. \n",
        "\n",
        "To get a Tf-idf matrix, first count word occurrences by document. This is transformed into a document-term matrix (dtm). This is also just called a term frequency matrix. An example of a dtm is here at right.\n",
        "\n",
        "Then apply the term frequency-inverse document frequency weighting: words that occur frequently within a document but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the document.\n",
        "\n",
        "A couple things to note about the parameters I define below:\n",
        "\n",
        "<ul>\n",
        "<li> max_df: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n",
        "<li> min_idf: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here I pass 0.2; the term must be in at least 20% of the document. I found that if I allowed a lower min_df I ended up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n",
        "<li> ngram_range: this just means I'll look at unigrams, bigrams and trigrams. See [n-grams](http://en.wikipedia.org/wiki/N-gram)\n",
        "</ul>"
      ],
      "metadata": {
        "id": "KKTGYmjPX5ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "\n",
        "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
        "\n",
        "print(tfidf_matrix.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.38 s, sys: 67.7 ms, total: 7.45 s\n",
            "Wall time: 7.5 s\n",
            "(100, 563)\n"
          ]
        }
      ],
      "metadata": {
        "id": "WbkJgpbXX5e0",
        "outputId": "f94fc75c-47a5-4332-c7a3-ca03187fdf95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "terms = tfidf_vectorizer.get_feature_names_out()"
      ],
      "outputs": [],
      "metadata": {
        "id": "pzybU1vpX5e0"
      },
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = 1 - cosine_similarity(tfidf_matrix)"
      ],
      "outputs": [],
      "metadata": {
        "id": "AAkUj3bRX5e0"
      },
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-means clustering"
      ],
      "metadata": {
        "id": "vk7-2hoEX5e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now onto the fun part. Using the tf-idf matrix, you can run a slew of clustering algorithms to better understand the hidden structure within the synopses. I first chose [k-means](http://en.wikipedia.org/wiki/K-means_clustering). K-means initializes with a pre-determined number of clusters (I chose 5). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and  centroids recalculated in an iterative process until the algorithm reaches convergence.\n",
        "\n",
        "I found it took several runs for the algorithm to converge a global optimum as k-means is susceptible to reaching local optima. "
      ],
      "metadata": {
        "id": "r6gOxQWnX5e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 5\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "\n",
        "%time km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 204 ms, sys: 112 ms, total: 316 ms\n",
            "Wall time: 198 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "metadata": {
        "id": "NX2WP0w8X5e1",
        "outputId": "fed8548f-4be7-4426-e126-dd1da2240350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "#joblib.dump(km,  'doc_cluster.pkl')\n",
        "\n",
        "!pip install scikit-learn==0.20.4\n",
        "km = joblib.load('doc_cluster.pkl')\n",
        "clusters = km.labels_.tolist()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn==0.20.4\n",
            "  Using cached scikit-learn-0.20.4.tar.gz (11.7 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.20.4) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==0.20.4) (1.10.1)\n",
            "Building wheels for collected packages: scikit-learn\n"
          ]
        }
      ],
      "metadata": {
        "id": "3G73DX6dX5e1",
        "outputId": "e07fcfa9-19d8-4f4f-83c8-252e679a0444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
        "\n",
        "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "AMBbBODFX5e1"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "frame['cluster'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {},
          "metadata": {},
          "execution_count": 18
        }
      ],
      "metadata": {
        "id": "pwX6bp63X5e1",
        "outputId": "b53722c9-4cc8-405c-ed58-dded760055b2"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = frame['rank'].groupby(frame['cluster'])\n",
        "\n",
        "grouped.mean()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {},
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "id": "nK8VziiXX5e1",
        "outputId": "5bf8467e-ae36-4fca-ab07-5269eab99e25"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "print()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    for ind in order_centroids[i, :6]:\n",
        "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Cluster %d titles:\" % i, end='')\n",
        "    for title in frame.ix[i]['title'].values.tolist():\n",
        "        print(' %s,' % title, end='')\n",
        "    print()\n",
        "    print()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top terms per cluster:\n",
            "\n",
            "Cluster 0 words: family"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", home, mother"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", war, house"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", dies,\n",
            "\n",
            "Cluster 0 titles: Schindler's List, One Flew Over the Cuckoo's Nest, Gone with the Wind, The Wizard of Oz, Titanic, Forrest Gump, E.T. the Extra-Terrestrial, The Silence of the Lambs, Gandhi, A Streetcar Named Desire, The Best Years of Our Lives, My Fair Lady, Ben-Hur, Doctor Zhivago, The Pianist, The Exorcist, Out of Africa, Good Will Hunting, Terms of Endearment, Giant, The Grapes of Wrath, Close Encounters of the Third Kind, The Graduate, Stagecoach, Wuthering Heights,\n",
            "\n",
            "Cluster 1 words: police"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", car, killed"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", murders, driving"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", house,\n",
            "\n",
            "Cluster 1 titles: Casablanca, Psycho, Sunset Blvd., Vertigo, Chinatown, Amadeus, High Noon, The French Connection, Fargo, Pulp Fiction, The Maltese Falcon, A Clockwork Orange, Double Indemnity, Rebel Without a Cause, The Third Man, North by Northwest,\n",
            "\n",
            "Cluster 2 words: father, new"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", york, new"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", brothers, apartments"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ",\n",
            "\n",
            "Cluster 2 titles: The Godfather, Raging Bull, Citizen Kane, The Godfather: Part II, On the Waterfront, 12 Angry Men, Rocky, To Kill a Mockingbird, Braveheart, The Good, the Bad and the Ugly, The Apartment, Goodfellas, City Lights, It Happened One Night, Midnight Cowboy, Mr. Smith Goes to Washington, Rain Man, Annie Hall, Network, Taxi Driver, Rear Window,\n",
            "\n",
            "Cluster 3 words: george, dance"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", singing, john"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", love, perform"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ",\n",
            "\n",
            "Cluster 3 titles: West Side Story, Singin' in the Rain, It's a Wonderful Life, Some Like It Hot, The Philadelphia Story, An American in Paris, The King's Speech, A Place in the Sun, Tootsie, Nashville, American Graffiti, Yankee Doodle Dandy,\n",
            "\n",
            "Cluster 4 words: killed, soldiers, captain"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ", men, army, command"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ",\n",
            "\n",
            "Cluster 4 titles: The Shawshank Redemption, Lawrence of Arabia, The Sound of Music, Star Wars, 2001: A Space Odyssey, The Bridge on the River Kwai, Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, Apocalypse Now, The Lord of the Rings: The Return of the King, Gladiator, From Here to Eternity, Saving Private Ryan, Unforgiven, Raiders of the Lost Ark, Patton, Jaws, Butch Cassidy and the Sundance Kid, The Treasure of the Sierra Madre, Platoon, Dances with Wolves, The Deer Hunter, All Quiet on the Western Front, Shane, The Green Mile, The African Queen, Mutiny on the Bounty,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "5H8-oxTjX5e1",
        "outputId": "7d6c3dbb-aecc-4252-9fa9-097f3e76c94a"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#This is purely to help export tables to html and to correct for my 0 start rank (so that Godfather is 1, not 0)\n",
        "frame['Rank'] = frame['rank'] + 1\n",
        "frame['Title'] = frame['title']"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z47w7CMXX5e2"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#export tables to HTML\n",
        "print(frame[['Rank', 'Title']].loc[frame['cluster'] == 1].to_html(index=False))"
      ],
      "outputs": [],
      "metadata": {
        "id": "j81_IbcdX5e2"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multidimensional scaling"
      ],
      "metadata": {
        "id": "fB_MfatuX5e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # for os.path.basename\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "MDS()\n",
        "\n",
        "# two components as we're plotting points in a two-dimensional plane\n",
        "# \"precomputed\" because we provide a distance matrix\n",
        "# we will also specify `random_state` so the plot is reproducible.\n",
        "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
        "\n",
        "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
        "\n",
        "xs, ys = pos[:, 0], pos[:, 1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "DmSj4dOTX5e2"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "def strip_proppers_POS(text):\n",
        "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
        "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
        "    return non_propernouns"
      ],
      "outputs": [],
      "metadata": {
        "id": "wFuEmfAZX5e3"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualizing document clusters"
      ],
      "metadata": {
        "id": "Y2KJOCQNX5e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set up colors per clusters using a dict\n",
        "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
        "\n",
        "#set up cluster names using a dict\n",
        "cluster_names = {0: 'Family, home, war', \n",
        "                 1: 'Police, killed, murders', \n",
        "                 2: 'Father, New York, brothers', \n",
        "                 3: 'Dance, singing, love', \n",
        "                 4: 'Killed, soldiers, captain'}"
      ],
      "outputs": [],
      "metadata": {
        "id": "VYl5k_4oX5e3"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "id": "qmeXmKjjX5e3"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')\n",
        "\n",
        "\n",
        "# set up plot\n",
        "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
        "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "#iterate through groups to layer the plot\n",
        "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
        "for name, group in groups:\n",
        "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
        "    ax.set_aspect('auto')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelbottom='off')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'y',         # changes apply to the y-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        left='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelleft='off')\n",
        "    \n",
        "ax.legend(numpoints=1)  #show legend with only 1 point\n",
        "\n",
        "#add label in x,y position with the label as the film title\n",
        "for i in range(len(df)):\n",
        "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
        "\n",
        "    \n",
        "    \n",
        "plt.show() #show the plot\n",
        "\n",
        "#uncomment the below to save the plot if need be\n",
        "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "QhbGPbfbX5e3",
        "outputId": "6f830e56-232e-4639-c258-72559d220456"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "7v7VkPi1X5e3"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering plot looks great, but it pains my eyes to see overlapping labels. Having some experience with [D3.js](http://d3js.org/) I knew one solution would be to use a browser based/javascript interactive. Fortunately, I recently stumbled upon [mpld3](https://mpld3.github.io/) a matplotlib wrapper for D3. Mpld3 basically let's you use matplotlib syntax to create web interactives. It has a really easy, high-level API for adding tooltips on mouse hover, which is what I am interested in.\n",
        "\n",
        "It also has some nice functionality for zooming and panning. The below javascript snippet basicaly defines a custom location for where the zoom/pan toggle resides. Don't worry about it too much and you actually don't need to use it, but it helped for formatting purposes when exporting to the web later. The only thing you might want to change is the x and y attr for the position of the toolbar."
      ],
      "metadata": {
        "id": "1np2KzpkX5e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define custom toolbar location\n",
        "class TopToolbar(mpld3.plugins.PluginBase):\n",
        "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
        "\n",
        "    JAVASCRIPT = \"\"\"\n",
        "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
        "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
        "    TopToolbar.prototype.constructor = TopToolbar;\n",
        "    function TopToolbar(fig, props){\n",
        "        mpld3.Plugin.call(this, fig, props);\n",
        "    };\n",
        "\n",
        "    TopToolbar.prototype.draw = function(){\n",
        "      // the toolbar svg doesn't exist\n",
        "      // yet, so first draw it\n",
        "      this.fig.toolbar.draw();\n",
        "\n",
        "      // then change the y position to be\n",
        "      // at the top of the figure\n",
        "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
        "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
        "\n",
        "      // then remove the draw function,\n",
        "      // so that it is not called again\n",
        "      this.fig.toolbar.draw = function() {}\n",
        "    }\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.dict_ = {\"type\": \"toptoolbar\"}"
      ],
      "outputs": [],
      "metadata": {
        "id": "qYajJM-IX5e4"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')\n",
        "\n",
        "#define custom css to format the font and to remove the axis labeling\n",
        "css = \"\"\"\n",
        "text.mpld3-text, div.mpld3-tooltip {\n",
        "  font-family:Arial, Helvetica, sans-serif;\n",
        "}\n",
        "\n",
        "g.mpld3-xaxis, g.mpld3-yaxis {\n",
        "display: none; }\n",
        "\"\"\"\n",
        "\n",
        "# Plot \n",
        "fig, ax = plt.subplots(figsize=(14,6)) #set plot size\n",
        "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "#iterate through groups to layer the plot\n",
        "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
        "for name, group in groups:\n",
        "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, label=cluster_names[name], mec='none', color=cluster_colors[name])\n",
        "    ax.set_aspect('auto')\n",
        "    labels = [i for i in group.title]\n",
        "    \n",
        "    #set tooltip using points, labels and the already defined 'css'\n",
        "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
        "                                       voffset=10, hoffset=10, css=css)\n",
        "    #connect tooltip to fig\n",
        "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
        "    \n",
        "    #set tick marks as blank\n",
        "    ax.axes.get_xaxis().set_ticks([])\n",
        "    ax.axes.get_yaxis().set_ticks([])\n",
        "    \n",
        "    #set axis as blank\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "    \n",
        "ax.legend(numpoints=1) #show legend with only one dot\n",
        "\n",
        "mpld3.display() #show the plot\n",
        "\n",
        "#uncomment the below to export to html\n",
        "#html = mpld3.fig_to_html(fig)\n",
        "#print(html)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {},
          "metadata": {},
          "execution_count": 58
        }
      ],
      "metadata": {
        "id": "5WYjEEFwX5e4",
        "outputId": "62b33552-8e1f-4e61-be0c-26502b237841"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hierarchical document clustering"
      ],
      "metadata": {
        "id": "GWs-zRGTX5e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "\n",
        "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
        "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
        "\n",
        "plt.tick_params(\\\n",
        "    axis= 'x',          # changes apply to the x-axis\n",
        "    which='both',      # both major and minor ticks are affected\n",
        "    bottom='off',      # ticks along the bottom edge are off\n",
        "    top='off',         # ticks along the top edge are off\n",
        "    labelbottom='off')\n",
        "\n",
        "plt.tight_layout() #show plot with tight layout\n",
        "\n",
        "#uncomment below to save figure\n",
        "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "jswzcrhJX5e4",
        "outputId": "f8940f72-1312-4cda-88e7-21851ad849f1"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "nQJOuERvX5e4"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Latent Dirichlet Allocation"
      ],
      "metadata": {
        "id": "vdG6l3ZzX5e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#strip any proper names from a text...unfortunately right now this is yanking the first word from a sentence too.\n",
        "import string\n",
        "def strip_proppers(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
        "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
      ],
      "outputs": [],
      "metadata": {
        "id": "UudjpB1DX5e5"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "def strip_proppers_POS(text):\n",
        "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
        "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
        "    return non_propernouns"
      ],
      "outputs": [],
      "metadata": {
        "id": "PtBiJ5VyX5e5"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Latent Dirichlet Allocation implementation with Gensim\n",
        "\n",
        "from gensim import corpora, models, similarities \n",
        "\n",
        "#remove proper names\n",
        "preprocess = [strip_proppers(doc) for doc in synopses]\n",
        "\n",
        "%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n",
        "\n",
        "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 15.4 s, sys: 142 ms, total: 15.6 s\n",
            "Wall time: 19 s\n",
            "CPU times: user 4.38 s, sys: 22.2 ms, total: 4.4 s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Wall time: 4.6 s\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "zSmlSp3MX5e5",
        "outputId": "3f16496c-9f1f-4695-fbcd-918c7b93e307"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#print(len([word for word in texts[0] if word not in stopwords]))\n",
        "print(len(texts[0]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1839\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "1phXvMmTX5e6",
        "outputId": "37411992-1262-48e2-c59d-088c9780b9bd"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(texts)"
      ],
      "outputs": [],
      "metadata": {
        "id": "v_XBNw21X5e6"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.filter_extremes(no_below=1, no_above=0.8)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UqAAcuP_X5e6"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "outputs": [],
      "metadata": {
        "id": "P3fSkZwzX5e6"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {},
          "metadata": {},
          "execution_count": 94
        }
      ],
      "metadata": {
        "id": "1UBt-mPEX5e6",
        "outputId": "23e2fae2-278d-486e-9f18-04d3cb6cce10"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 48s, sys: 3.98 s, total: 9min 52s\n",
            "Wall time: 11min 43s\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "2DDvG_l1X5e7",
        "outputId": "8b667389-7bb2-4ba3-cfcd-733dccab6d88"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda[corpus[0]])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 0.14010786617821663), (2, 0.2389980721391618), (3, 0.080853026271803449), (4, 0.53991713521730689)]\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "qLqFudtcX5e7",
        "outputId": "32e6d05e-2d1f-47bd-90ab-7d296397d259"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topics = lda.print_topics(5, num_words=20)"
      ],
      "outputs": [],
      "metadata": {
        "id": "r6F1VvCZX5e7"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topics_matrix = lda.show_topics(formatted=False, num_words=20)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NMhpm9AJX5e7"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topics_matrix = np.array(topics_matrix)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yNwv6CrxX5e7"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topics_matrix.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {},
          "metadata": {},
          "execution_count": 145
        }
      ],
      "metadata": {
        "id": "AvfijGq4X5e7",
        "outputId": "774756af-bd3f-4137-ccce-9d64c80b5b28"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words = topics_matrix[:,:,1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "IuD0ERVaX5e8"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for i in topic_words:\n",
        "    print([str(word) for word in i])\n",
        "    print()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['famili', 'murder', 'prison', 'work', 'kill', 'life', 'guard', 'way', \"n't\", 'two', 'meet', 'would', 'death', 'say', 'll', 'vote', 'becom', 'ask', 'guilti', 'peopl']\n",
            "\n",
            "['love', 'marri', 'go', 'home', 'friend', 'show', 'day', 'father', 'meet', 'want', \"n't\", 'night', 'doe', 'apart', 'famili', 'work', 'film', 'first', 'sing', 'come']\n",
            "\n",
            "['kill', 'meet', 'arriv', 'first', 'two', 'order', 'escap', 'friend', 'say', 'famili', 'ask', 'call', 'attempt', 'father', 'later', 'refus', \"n't\", 'offer', 'name', 'away']\n",
            "\n",
            "['kill', 'soldier', 'men', 'order', 'shark', 'command', 'attack', 'water', 'war', 'offic', 'arriv', 'boat', 'die', 'wound', 'two', 'attempt', 'villag', 'fire', 'battl', 'shoot']\n",
            "\n",
            "['car', 'ask', 'polic', 'home', \"n't\", 'go', 'say', 'kill', 'run', 'hous', 'goe', 'two', 'friend', 'day', 'call', 'come', 'drive', 'meet', 'doe', 'away']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "aW7ubQMqX5e8",
        "outputId": "29a01391-4e1f-471c-8c49-3f629df188c0"
      },
      "execution_count": null
    }
  ]
}